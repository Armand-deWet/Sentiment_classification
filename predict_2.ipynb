{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c8cc4c",
   "metadata": {},
   "source": [
    "# Climate Change Classification - Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd88501",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02028cd2",
   "metadata": {},
   "source": [
    "In this predict I will try and classify Twitter users sentiment regarding climate change using machine learning models. The different classes to consider are 2: which are links to factual news regarding climate change, 1: supports belief of man-made climate change, 0: neither support or refutes (neutral), -1: does npd believe in man-made climate change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663fee6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9dca05e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set()\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.utils import resample\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e26a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9b78ed2",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7fdb8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\") # this is the testing data, i.e. tweets without the sentiment label\n",
    "train = pd.read_csv(\"train.csv\") # this is the testing data, i.e tweets with the sentiment label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc9702",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72a33796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @nycjim: Trump muzzles employees of several...</td>\n",
       "      <td>75639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@bmastenbrook yes wrote that in 3rd yr Comp Sc...</td>\n",
       "      <td>211536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @climatehawk1: Indonesian farmers weather #...</td>\n",
       "      <td>569434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @guardian: British scientists face a ‘huge ...</td>\n",
       "      <td>315368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aid For Agriculture | Sustainable agriculture ...</td>\n",
       "      <td>591733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928\n",
       "5  RT @nycjim: Trump muzzles employees of several...    75639\n",
       "6  @bmastenbrook yes wrote that in 3rd yr Comp Sc...   211536\n",
       "7  RT @climatehawk1: Indonesian farmers weather #...   569434\n",
       "8  RT @guardian: British scientists face a ‘huge ...   315368\n",
       "9  Aid For Agriculture | Sustainable agriculture ...   591733"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44e4dd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344becf",
   "metadata": {},
   "source": [
    "I suspect the data may be imbalanced, so lets take a look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a257f8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 0, -1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type_labels = list(train_df.sentiment.unique())\n",
    "print(type_labels)\n",
    "train_df['sentiment'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22bc799",
   "metadata": {},
   "source": [
    "Clearly the dominant class by a large margin is 1. I will proceed using two techniques to try and combat this. I will use SMOTE to equalize the amount of the different labels and for the other approach I will simply try and train the different models using the class_weight = balanced hyperparamter, where applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167fc79",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc07439",
   "metadata": {},
   "source": [
    "Cleaning the data in order to eliminate noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9435a1c0",
   "metadata": {},
   "source": [
    "### Function to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d940797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_message(tweet):\n",
    "    # convert to lower case\n",
    "    tweet = tweet.lower() \n",
    "    \n",
    "    # Remove mentions   \n",
    "    tweet = re.sub('@[\\w]*','',tweet)  \n",
    "    \n",
    "    # Remove url's\n",
    "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r'#\\w*', '', tweet)    \n",
    "    \n",
    "    # Remove numbers\n",
    "    tweet = re.sub(r'\\d+', '', tweet)  \n",
    "    \n",
    "    # Remove punctuation\n",
    "    tweet = re.sub(r\"[,.;':@#?!\\&/$]+\\ *\", ' ', tweet)\n",
    "    \n",
    "    # Remove diamond\n",
    "    tweet = re.sub(r\"U+FFFD \", ' ', tweet)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
    "    \n",
    "    # Remove space in front of tweet\n",
    "    tweet = tweet.lstrip(' ')                        \n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f56871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] =train['message'].apply(cleaned_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b0e1bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesn t think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>it s not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt researchers say we have three years to act ...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>wired was a pivotal year in the war on climate...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt it s and a racist sexist climate change den...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>worth a read whether you do or don t believe i...</td>\n",
       "      <td>425577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>rt mike pence doesn’t believe in global warmin...</td>\n",
       "      <td>294933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>rt six big things we can all do today to fight...</td>\n",
       "      <td>992717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>my yo nephew is inconsolable he wants to die o...</td>\n",
       "      <td>664510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>rt no offense… but like… how do you just not b...</td>\n",
       "      <td>260471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  polyscimajor epa chief doesn t think carbon di...   625221\n",
       "1          1  it s not like we lack evidence of anthropogeni...   126103\n",
       "2          2  rt researchers say we have three years to act ...   698562\n",
       "3          1  wired was a pivotal year in the war on climate...   573736\n",
       "4          1  rt it s and a racist sexist climate change den...   466954\n",
       "5          1  worth a read whether you do or don t believe i...   425577\n",
       "6          1  rt mike pence doesn’t believe in global warmin...   294933\n",
       "7          1  rt six big things we can all do today to fight...   992717\n",
       "8          1  my yo nephew is inconsolable he wants to die o...   664510\n",
       "9          1  rt no offense… but like… how do you just not b...   260471"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39031ad4",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da953972",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d44230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] = train['message'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ea47f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[polyscimajor, epa, chief, doesn, t, think, ca...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[it, s, not, like, we, lack, evidence, of, ant...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[rt, researchers, say, we, have, three, years,...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[wired, was, a, pivotal, year, in, the, war, o...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[rt, it, s, and, a, racist, sexist, climate, c...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  [polyscimajor, epa, chief, doesn, t, think, ca...   625221\n",
       "1          1  [it, s, not, like, we, lack, evidence, of, ant...   126103\n",
       "2          2  [rt, researchers, say, we, have, three, years,...   698562\n",
       "3          1  [wired, was, a, pivotal, year, in, the, war, o...   573736\n",
       "4          1  [rt, it, s, and, a, racist, sexist, climate, c...   466954"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2222d84",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6cbf89ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Armand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Armand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f6d8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14eb8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] = train['message'].apply(df_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6626c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] = [' '.join(map(str, l)) for l in train['message']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f499e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesn t think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>it s not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt researcher say we have three year to act on...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>wired wa a pivotal year in the war on climate ...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt it s and a racist sexist climate change den...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>worth a read whether you do or don t believe i...</td>\n",
       "      <td>425577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>rt mike penny doesn ’ t believe in global warm...</td>\n",
       "      <td>294933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>rt six big thing we can all do today to fight ...</td>\n",
       "      <td>992717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>my yo nephew is inconsolable he want to die of...</td>\n",
       "      <td>664510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>rt no offense… but like… how do you just not b...</td>\n",
       "      <td>260471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  polyscimajor epa chief doesn t think carbon di...   625221\n",
       "1          1  it s not like we lack evidence of anthropogeni...   126103\n",
       "2          2  rt researcher say we have three year to act on...   698562\n",
       "3          1  wired wa a pivotal year in the war on climate ...   573736\n",
       "4          1  rt it s and a racist sexist climate change den...   466954\n",
       "5          1  worth a read whether you do or don t believe i...   425577\n",
       "6          1  rt mike penny doesn ’ t believe in global warm...   294933\n",
       "7          1  rt six big thing we can all do today to fight ...   992717\n",
       "8          1  my yo nephew is inconsolable he want to die of...   664510\n",
       "9          1  rt no offense… but like… how do you just not b...   260471"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca2851",
   "metadata": {},
   "source": [
    "## Model building and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9946026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['message']\n",
    "y = train['sentiment']\n",
    "\n",
    "# Split the train data to create validation dataset\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b857c64b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'csr_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m TFIDF \u001b[38;5;241m=\u001b[39m TfidfVectorizer(preprocessor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m, ngram_range \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m), min_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, strip_accents \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m'\u001b[39m, smooth_idf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m vec_train \u001b[38;5;241m=\u001b[39m \u001b[43mTFIDF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m vec_val \u001b[38;5;241m=\u001b[39m TFIDF\u001b[38;5;241m.\u001b[39mfit(X_valid)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2049\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2044\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2045\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2046\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2047\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2048\u001b[0m )\n\u001b[1;32m-> 2049\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2050\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1211\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1211\u001b[0m         feature_idx \u001b[38;5;241m=\u001b[39m \u001b[43mvocabulary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1212\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m feature_idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m feature_counter:\n\u001b[0;32m   1213\u001b[0m             feature_counter[feature_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'csr_matrix'"
     ]
    }
   ],
   "source": [
    "TFIDF = TfidfVectorizer(preprocessor = list, tokenizer = list, ngram_range = (1,2), min_df = 2, strip_accents = 'ascii', smooth_idf = False)\n",
    "vec_train = TFIDF.fit(X_train)\n",
    "vec_val = TFIDF.fit(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c00aa0e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'csr_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mTFIDF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m X_tr \u001b[38;5;241m=\u001b[39m TFIDF\u001b[38;5;241m.\u001b[39mtransform(X_train)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2103\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2086\u001b[0m \u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2087\u001b[0m \n\u001b[0;32m   2088\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2101\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2103\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1389\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1211\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1211\u001b[0m         feature_idx \u001b[38;5;241m=\u001b[39m \u001b[43mvocabulary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1212\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m feature_idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m feature_counter:\n\u001b[0;32m   1213\u001b[0m             feature_counter[feature_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'csr_matrix'"
     ]
    }
   ],
   "source": [
    "X_train = TFIDF.transform(X_train)\n",
    "X_tr = TFIDF.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "35d4679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "               ('clf', RandomForestClassifier(max_depth=5, \n",
    "                                              n_estimators=100))])\n",
    "\n",
    "# Naïve Bayes:\n",
    "nb = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "               ('clf', MultinomialNB())])\n",
    "\n",
    "# K-NN Classifier\n",
    "knn = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                ('clf', KNeighborsClassifier(n_neighbors=5, \n",
    "                                             metric='minkowski', \n",
    "                                             p=2))])\n",
    "\n",
    "# Logistic Regression\n",
    "lr = Pipeline([('tfidf',TfidfVectorizer()),\n",
    "               ('clf',LogisticRegression(C=1, \n",
    "                                         class_weight='balanced', \n",
    "                                         max_iter=1000))])\n",
    "# Linear SVC:\n",
    "lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                 ('clf', LinearSVC(class_weight='balanced'))])\n",
    "\n",
    "# RBF SVC\n",
    "rbfsvc = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                 ('clf', SVC(class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d73183c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest \n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_valid)\n",
    "\n",
    "# Niave bayes\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_valid)\n",
    "\n",
    "# K - nearest neighbors\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_valid)\n",
    "\n",
    "# Linear regression\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_valid)\n",
    "\n",
    "# Linear SVC\n",
    "lsvc.fit(X_train, y_train)\n",
    "y_pred_lsvc = lsvc.predict(X_valid)\n",
    "\n",
    "# RBF SVC\n",
    "rbfsvc.fit(X_train, y_train)\n",
    "y_pred_rbfsvc = rbfsvc.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02720435",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "names = ['Random Forest', 'Naïve Bayes',\n",
    "         'K-NN', 'Logistic Regression', 'Linear SVC',\n",
    "         'RBF SVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "12440152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       336\n",
      "           0       0.00      0.00      0.00       547\n",
      "           1       0.55      1.00      0.71      2178\n",
      "           2       0.00      0.00      0.00       894\n",
      "\n",
      "    accuracy                           0.55      3955\n",
      "   macro avg       0.14      0.25      0.18      3955\n",
      "weighted avg       0.30      0.55      0.39      3955\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armand\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Armand\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Armand\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_valid, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fef4836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       336\n",
      "           0       0.93      0.05      0.09       547\n",
      "           1       0.61      0.98      0.76      2178\n",
      "           2       0.88      0.44      0.58       894\n",
      "\n",
      "    accuracy                           0.65      3955\n",
      "   macro avg       0.61      0.37      0.36      3955\n",
      "weighted avg       0.67      0.65      0.56      3955\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armand\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Armand\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Armand\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_valid, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e14c9ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.11      0.20       336\n",
      "           0       0.81      0.14      0.24       547\n",
      "           1       0.92      0.22      0.36      2178\n",
      "           2       0.26      0.96      0.41       894\n",
      "\n",
      "    accuracy                           0.37      3955\n",
      "   macro avg       0.73      0.36      0.30      3955\n",
      "weighted avg       0.76      0.37      0.34      3955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_valid, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "298a7f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.48      0.65      0.55       336\n",
      "           0       0.43      0.57      0.49       547\n",
      "           1       0.86      0.67      0.75      2178\n",
      "           2       0.68      0.83      0.74       894\n",
      "\n",
      "    accuracy                           0.69      3955\n",
      "   macro avg       0.61      0.68      0.63      3955\n",
      "weighted avg       0.73      0.69      0.70      3955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_valid, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "84fc1121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.51      0.54      0.53       336\n",
      "           0       0.50      0.50      0.50       547\n",
      "           1       0.80      0.77      0.79      2178\n",
      "           2       0.72      0.77      0.74       894\n",
      "\n",
      "    accuracy                           0.71      3955\n",
      "   macro avg       0.63      0.65      0.64      3955\n",
      "weighted avg       0.72      0.71      0.71      3955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_valid, y_pred_lsvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "99e5b187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.70      0.49      0.57       336\n",
      "           0       0.49      0.54      0.51       547\n",
      "           1       0.83      0.78      0.80      2178\n",
      "           2       0.70      0.84      0.77       894\n",
      "\n",
      "    accuracy                           0.73      3955\n",
      "   macro avg       0.68      0.66      0.66      3955\n",
      "weighted avg       0.74      0.73      0.73      3955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_valid, y_pred_rbfsvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "95a1bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.58      0.56      0.57       336\n",
      "           0       0.53      0.51      0.52       547\n",
      "           1       0.82      0.80      0.81      2178\n",
      "           2       0.73      0.80      0.76       894\n",
      "\n",
      "    accuracy                           0.74      3955\n",
      "   macro avg       0.66      0.67      0.67      3955\n",
      "weighted avg       0.74      0.74      0.74      3955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rbf_op = Pipeline([('tfidf', TfidfVectorizer(max_df=0.8,\n",
    "                                                    min_df=2,\n",
    "                                                    ngram_range=(1,2))),\n",
    "                  ('clf', LinearSVC(C=0.3,\n",
    "                                    class_weight='balanced',\n",
    "                                    max_iter=3000))])\n",
    "\n",
    "rbf_op.fit(X_train, y_train)\n",
    "y_pred = rbf_op.predict(X_valid)\n",
    "\n",
    "print(metrics.classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30db55c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=100),\n",
    "    MultinomialNB(),\n",
    "    KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2),\n",
    "    LogisticRegression(C=1, class_weight='balanced', max_iter=1000),\n",
    "    SVC(kernel = 'linear', class_weight='balanced'),\n",
    "    SVC(kernel = 'RBF', class_weight='balanced')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea4c1fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Random Forest model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(names, classifiers):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m model...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m---> 10\u001b[0m     run_time \u001b[38;5;241m=\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-q -o clf.fit(X_train, y_train)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m... predicting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2294\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2292\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2293\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2294\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py:1162\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m   1161\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m index\n\u001b[1;32m-> 1162\u001b[0m     time_number \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[0;32m   1164\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py:156\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    154\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[1;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:331\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 331\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    594\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1069\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1071\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1072\u001b[0m     )\n\u001b[1;32m-> 1074\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1090\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py:872\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;124;03m    Return the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m          dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "models = {}\n",
    "confusion = {}\n",
    "class_report = {}\n",
    "\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print ('Fitting {:s} model...'.format(name))\n",
    "    run_time = %timeit -q -o clf.fit(X_train, y_train)\n",
    "\n",
    "    print ('... predicting')\n",
    "    y_pred = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_valid)\n",
    "\n",
    "    print ('... scoring')\n",
    "\n",
    "    f1        = metrics.f1_score(y_train, y_pred, average = 'macro')\n",
    "    f1_test   = metrics.f1_score(y_valid, y_pred_test, average = 'macro')\n",
    "\n",
    "    # Save the results to dictionaries\n",
    "    models[name] = clf\n",
    "    confusion[name] = metrics.confusion_matrix(y_train, y_pred)\n",
    "    class_report[name] = metrics.classification_report(y_valid, y_pred)\n",
    "\n",
    "    results.append([name, f1, f1_test, run_time.best])\n",
    "\n",
    "\n",
    "results = pd.DataFrame(results, columns=['Classifier', 'F1 Train', 'F1 Test', 'Train Time'])\n",
    "results.set_index('Classifier', inplace= True)\n",
    "\n",
    "print ('... All done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2a63331e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Train</th>\n",
       "      <th>F1 Test</th>\n",
       "      <th>Train Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RBF SVM</th>\n",
       "      <td>0.991781</td>\n",
       "      <td>0.666446</td>\n",
       "      <td>17.327767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.927319</td>\n",
       "      <td>0.635528</td>\n",
       "      <td>0.911477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear SVM</th>\n",
       "      <td>0.953561</td>\n",
       "      <td>0.630966</td>\n",
       "      <td>12.478132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sigmoid SVM</th>\n",
       "      <td>0.884574</td>\n",
       "      <td>0.607672</td>\n",
       "      <td>13.958012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.999047</td>\n",
       "      <td>0.601588</td>\n",
       "      <td>43.423481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.999047</td>\n",
       "      <td>0.516678</td>\n",
       "      <td>3.090721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.560130</td>\n",
       "      <td>0.516258</td>\n",
       "      <td>3.349231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nearest Neighbors</th>\n",
       "      <td>0.801936</td>\n",
       "      <td>0.476397</td>\n",
       "      <td>0.000869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     F1 Train   F1 Test  Train Time\n",
       "Classifier                                         \n",
       "RBF SVM              0.991781  0.666446   17.327767\n",
       "Logistic Regression  0.927319  0.635528    0.911477\n",
       "Linear SVM           0.953561  0.630966   12.478132\n",
       "Sigmoid SVM          0.884574  0.607672   13.958012\n",
       "Random Forest        0.999047  0.601588   43.423481\n",
       "Decision Tree        0.999047  0.516678    3.090721\n",
       "AdaBoost             0.560130  0.516258    3.349231\n",
       "Nearest Neighbors    0.801936  0.476397    0.000869"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values('F1 Test', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b8135",
   "metadata": {},
   "source": [
    "The RBF SVM, Logistics regression and Linear SVM performed the best, I will thus tryand tune their hyper parameters to see if I can improve their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb687afc",
   "metadata": {},
   "source": [
    "## Hyperparamter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a27984",
   "metadata": {},
   "source": [
    "### RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "941ad2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RBF_SVM = SVC()\n",
    "param_grid = {'gamma': [1, 0.1, 0.01, 0.001, 0.0001]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1760de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(RBF_SVM, param_grid, scoring = 'f1_macro', refit = True)\n",
    "grid.fit(X_resampled_train_V, y_resampled_train)\n",
    "\n",
    "#RBF_SVM.fit(X_resampled_train_V, y_resampled_train)\n",
    "pred = grid.predict(X_SMOTE_test_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "62b9199a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 1}\n",
      "SVC(gamma=1)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params_)\n",
    "  \n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "65e46421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6664460916701628\n"
     ]
    }
   ],
   "source": [
    "#grid_pred = grid.predict(X_SMOTE_test_V)\n",
    "f1_test   = metrics.f1_score(y_SMOTE_test, pred, average = 'macro')\n",
    "print(f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c6e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
